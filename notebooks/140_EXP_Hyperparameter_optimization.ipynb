{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in Jupyter's Terminal\n",
    "# pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import Trials\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from data_utils import get_train_test_split_for_stock\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_split_for_stock(PATH_TO_DATA_FILE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The way optimization works in hyperopt is through minimization, but in our\n",
    "case, we want the maximum possible F1 score metric. So, the way we define our\n",
    "loss (the function to minimize) is as the inverse of our F1 score metric, as in\n",
    "loss = 1 - f-score, so the minimization of this function will represent the best\n",
    "F1 score metric.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def objective(params, X_train=X_train, y_train=y_train, random_state=RANDOM_STATE, n_folds=N_FOLDS):\n",
    "    \"\"\"\n",
    "    Objective function for Logistic Regression Hyperparameter Tuning\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evaluate based on ROC AUC\n",
    "    \n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        \n",
    "        clf = LogisticRegression(**params, random_state=random_state, verbose=0)\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=n_folds, scoring='f1_macro')\n",
    "\n",
    "        # Extract the best score\n",
    "        best_score = max(scores)\n",
    "\n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "\n",
    "        # Dictionary with information for evaluation\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'warm_start' : hp.choice('warm_start', [True, False]),\n",
    "    'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "    'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "    'C' : hp.uniform('C', 0.05, 3),\n",
    "    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "    'max_iter' : hp.choice('max_iter', range(5,1000))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"SP_EXP_HyperParam_Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Trials object to track progress\n",
    "bayes_trials = Trials()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Hyperopt optimization\"):\n",
    "    \n",
    "    best = fmin(fn = objective, \n",
    "                space = space, \n",
    "                algo = tpe.suggest, \n",
    "                max_evals = MAX_EVALS, \n",
    "                trials = bayes_trials)\n",
    "    \n",
    "    mlflow.log_param(\"Best params\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show\n",
    "- nested experiments in UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
