{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mlops_training_data_pipeline.png\" width=1000/>\n",
    "\n",
    "## Data Pipeline\n",
    "Our data pipeline will run daily, collecting data and storing it in a way that can be run by machine learning jobs.\n",
    "\n",
    "We will structure our data folders into three types of data:\n",
    "- **raw** -> Get data: retrieve data from the API and save as a CSV file (data/raw/data.csv)\n",
    "- **staging** -> Check and verify data: run quality checks, schema verification, and confirm that the data can be used in production (data/staging/data.csv)\n",
    "- **training** -> Generate feature sets: the final product of the data pipeline, contains the data processed into features that can be consumed directly by the training process (data/training/data.csv)\n",
    "\n",
    "With these three distinct phases, we ensure the reproducibility of the training data generation process, visibility, and a clear separation of the different steps of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from data_utils import get_train_test_split_for_stock\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#_path_to_src = \"/home/ksatola/work/src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to PATH_TO_DATA_PIPELINE\n",
    "os.chdir(PATH_TO_DATA_PIPELINE) \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create the folder if does not exist\n",
    "Path(PATH_TO_DATA_PIPELINE).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to PATH_TO_DATA_PIPELINE\n",
    "os.chdir(PATH_TO_DATA_PIPELINE) \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Project\n",
    "The MLflow projects feature allows your project to run in advanced cloud environments such as `Kubernetes` and `Databricks`. Scaling your ML job seamlessly is one of the main selling points of a platform such as MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the MLProject file\n",
    "_mlproject = \"MLProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_mlproject}\n",
    "\n",
    "name: training_data_pipeline\n",
    "\n",
    "conda_env:\n",
    "    \n",
    "    conda.yaml\n",
    "\n",
    "entry_points: \n",
    "    \n",
    "    load_raw_data:\n",
    "        command: \"python load_raw_data.py\"\n",
    "            \n",
    "    clean_validate_data:\n",
    "        command: \"python check_verify_data.py\"\n",
    "            \n",
    "    feature_set_generation:\n",
    "        command: \"python generate_feature_set.py\"\n",
    "            \n",
    "    main:\n",
    "        command: \"python main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the conda.yaml file\n",
    "_conda = \"conda.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.blog/2021-09-01-improving-git-protocol-security-github/\n",
    "# git+git://github.com/mlflow/mlflow is no longer working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_conda}\n",
    "\n",
    "name: pystock-data-features\n",
    "channels:\n",
    "    - defaults\n",
    "dependencies:\n",
    "    - python=3.8\n",
    "    - numpy\n",
    "    - scipy\n",
    "    - pandas\n",
    "    - cloudpickle\n",
    "    - pip\n",
    "    - pip:\n",
    "        - git+https://github.com/mlflow/mlflow\n",
    "        - pandas_datareader\n",
    "        - great-expectations\n",
    "        - pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the main.py file\n",
    "_main = \"main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_main}\n",
    "\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def _run(entrypoint, \n",
    "         parameters={}, \n",
    "         source_version=None, \n",
    "         use_cache=True):\n",
    "    \n",
    "    print(\"---------------------\")\n",
    "    print(f\"Launching new run for entrypoint={entrypoint} and parameters={parameters}\")\n",
    "    submitted_run = mlflow.run(\".\", entrypoint, parameters=parameters)\n",
    "    return submitted_run\n",
    "\n",
    "\n",
    "def workflow():\n",
    "    \n",
    "    with mlflow.start_run(run_name =\"training-data-pipeline\") as active_run:\n",
    "        \n",
    "        mlflow.set_tag(\"mlflow.runName\", \"training-data-pipeline\")\n",
    "        \n",
    "        _run(\"load_raw_data\")\n",
    "        _run(\"clean_validate_data\")\n",
    "        _run(\"feature_set_generation\")\n",
    "        \n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data\n",
    "Load the data from the API and save it in the _raw_ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and save it in the _raw_ folder\n",
    "_load_raw_data = \"load_raw_data.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_load_raw_data}\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PATH_TO_CONFIG = \"/home/ksatola/work/src\"\n",
    "sys.path.insert(1, PATH_TO_CONFIG)\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "from config import *\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Workaround to handle issue https://github.com/pydata/pandas-datareader/issues/868\n",
    "    USER_AGENT = {\n",
    "        'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "                    ' Chrome/91.0.4472.124 Safari/537.36')\n",
    "        }\n",
    "    sesh = requests.Session()\n",
    "    sesh.headers.update(USER_AGENT)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"load_raw_data\") as run:\n",
    "\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"training-data-pipeline\")\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"load_raw_data\")\n",
    "        end = date.today()\n",
    "        start = end + relativedelta(months=-3)\n",
    "        \n",
    "        df = web.DataReader(\"BTC-USD\", 'yahoo', start, end, session=sesh)\n",
    "        \n",
    "        # Create the folder if does not exist\n",
    "        Path(PATH_TO_DATA_PIPELINE, \"raw\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #df.to_csv(\"/home/ksatola/work/data/raw/data.csv\")\n",
    "        df.to_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"raw\", \"data.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality check\n",
    "Checking data quality as part of your machine learning system is extremely critical to ensure the integrity and correctness of your model training and inference. \n",
    "\n",
    "From a data quality perspective, in a dataset there are a couple of critical dimensions with which to assess and profile our data, namely:\n",
    "- **Schema compliance:** Ensuring the data is from the expected types; making sure that numeric values don't contain any other types of data\n",
    "- **Valid data:** Assessing from a data perspective whether the data is valid from a business perspective\n",
    "- **Missing data:** Assessing whether all the data needed to run analytics and algorithms is available\n",
    "\n",
    "`Great Expectations Python package` for data validation, reference: https://github.com/great-expectations/great_expectations\n",
    "\n",
    "#### In our project\n",
    "We want the following rules/constraints to be verifiable:\n",
    "- Date values should be valid dates and cannot be missing\n",
    "- Check numeric and long values are correctly typed\n",
    "- All columns are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_check_verify_data = \"check_verify_data.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_check_verify_data}\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PATH_TO_CONFIG = \"/home/ksatola/work/src\"\n",
    "sys.path.insert(1, PATH_TO_CONFIG)\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "import json\n",
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.profile.basic_dataset_profiler import BasicDatasetProfiler\n",
    "from config import *\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"check_verify_data\") as run:\n",
    "\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"training-data-pipeline\")\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"check_verify_data\")\n",
    "\n",
    "        #df = pd.read_csv(\"/home/ksatola/work/data/raw/data.csv\")\n",
    "        df = pd.read_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"raw\", \"data.csv\"), index_col=\"Date\")\n",
    "        \n",
    "        # Convert Date index to column for data signature checking\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        describe_to_dict = df.describe().to_dict()\n",
    "        mlflow.log_dict(describe_to_dict, \"describe_data.json\")\n",
    "        \n",
    "        print(json.dumps(\n",
    "            describe_to_dict,\n",
    "            sort_keys=True,\n",
    "            indent=4,\n",
    "            separators=(',', ': ')\n",
    "        ))\n",
    "        \n",
    "        pd_df_ge = ge.from_pandas(df)\n",
    "        \n",
    "        #print(20*\"--\")\n",
    "        #print(pd_df_ge)\n",
    "\n",
    "        # Data signature evaluation\n",
    "        assert pd_df_ge.expect_column_values_to_match_strftime_format(\"Date\", \"%Y-%m-%d\").success == True\n",
    "        assert pd_df_ge.expect_column_values_to_be_of_type(\"High\", \"float\").success == True\n",
    "        assert pd_df_ge.expect_column_values_to_be_of_type(\"Low\", \"float\").success == True\n",
    "        assert pd_df_ge.expect_column_values_to_be_of_type(\"Open\", \"float\").success == True\n",
    "        #assert pd_df_ge.expect_column_values_to_be_of_type(\"Open\", \"string\").success == True\n",
    "        assert pd_df_ge.expect_column_values_to_be_of_type(\"Close\", \"float\").success == True\n",
    "        #assert pd_df_ge.expect_column_values_to_be_of_type(\"Volume\", \"float\").success == True\n",
    "        assert pd_df_ge.expect_column_values_to_be_of_type(\"Volume\", \"long\").success == True\n",
    "        assert pd_df_ge.expect_column_values_to_be_of_type(\"Adj Close\", \"float\").success == True\n",
    "\n",
    "        # We can do some basic cleaning by dropping the null values\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Create the folder if does not exist\n",
    "        Path(PATH_TO_DATA_PIPELINE, \"staging\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #df.to_csv(\"/home/ksatola/work/data/staging/data.csv\")\n",
    "        df.to_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"staging\", \"data.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for modelling\n",
    "The feature_set_generation.py file, will be responsible for generating our features and saving them in the training folder where all the data is valid and ready to be used for ML training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_feature_set = \"generate_feature_set.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_generate_feature_set}\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PATH_TO_CONFIG = \"/home/ksatola/work/src\"\n",
    "sys.path.insert(1, PATH_TO_CONFIG)\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import *\n",
    "\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    \"\"\"\n",
    "    Takes np.array 'a' and size 'window' as parameters \n",
    "    Outputs an np.array with all the ordered sequences of values of 'a' of size 'window'\n",
    "        e.g. Input: ( np.array([1, 2, 3, 4, 5, 6]), 4 )\n",
    "             Output: \n",
    "                     array([[1, 2, 3, 4],\n",
    "                           [2, 3, 4, 5],\n",
    "                           [3, 4, 5, 6]])\n",
    "    \"\"\"\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"training-data-pipeline\")\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"generate_feature_set\")\n",
    "\n",
    "        #raise Exception(f\"Here the exception message\")\n",
    "\n",
    "        #df = pd.read_csv(\"/home/ksatola/work/data/staging/data.csv\")\n",
    "        df = pd.read_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"staging\", \"data.csv\"))\n",
    "\n",
    "        df['Delta Pct'] = (df['Close'] - df['Open'])/df['Open']\n",
    "        df['Going Up'] = df['Delta Pct'].apply(lambda d: 1 if d > 0.00001 else 0)\n",
    "                       \n",
    "        # t-10 | t-9 | ... | t-2 | t-1 | label (Going Up)\n",
    "        training_dataset = rolling_window(df['Going Up'].to_numpy(), WINDOW_SIZE)                 \n",
    "                         \n",
    "        cols = [\"t-{}\".format(10-i) for i in range(0, 10)] + [\"target\"]\n",
    "        df = pd.DataFrame(training_dataset, columns=cols)   \n",
    "        \n",
    "        # Create the folder if does not exist\n",
    "        Path(PATH_TO_DATA_PIPELINE, \"training\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        df.to_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"training\", \"data.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the HOST terminal run\n",
    "# git config --global url.\"https://github.com/\".insteadOf git://github.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Jupyter Terminal\n",
    "cd /home/ksatola/work/data/data_pipeline\n",
    "\n",
    "# If there is conda environment error, than it may be too less RAM memory assigned for Docker\n",
    "mlflow run . --experiment-name=\"SP_Training_Data_Pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
