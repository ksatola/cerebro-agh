{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Pipeline\n",
    "\n",
    "<img src=\"img/mlops_training_data_pipeline.png\" width=1000/>\n",
    "\n",
    "Our training should run periodically or when triggered by a dataset arrival. The main output of the training project is a new model that is generated as output and registered in the Model Registry with different details.\n",
    "\n",
    "The pipeline steps are:\n",
    "- **Train**\n",
    "- **Evaluate**\n",
    "- **Register**\n",
    "    \n",
    "With these three distinct phases, we ensure reproducibility of the model training process and visibility and clear separation of the different steps of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from data_utils import get_train_test_split_for_stock\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda update -n base conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH_TO_TRAINING) \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create the folder if does not exist\n",
    "Path(PATH_TO_TRAINING).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLProject file\n",
    "_mlproject = \"MLProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_mlproject}\n",
    "\n",
    "name: model_training_pipeline\n",
    "\n",
    "conda_env: \n",
    "    conda.yaml\n",
    "\n",
    "entry_points:\n",
    "\n",
    "  main:\n",
    "    command: \"python main.py\"\n",
    "\n",
    "  train_model:\n",
    "    command: \"python train_model.py\"\n",
    "\n",
    "  evaluate_model:\n",
    "    command: \"python evaluate_model.py\"\n",
    "\n",
    "  register_model:\n",
    "    parameters:\n",
    "      model_uri: string\n",
    "    command: \"python register_model.py {model_uri}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conda.yaml file\n",
    "_conda = \"conda.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_conda}\n",
    "\n",
    "name: pystock-data-features\n",
    "channels:\n",
    "    - defaults\n",
    "dependencies:\n",
    "    - python=3.8\n",
    "    - numpy\n",
    "    - scipy\n",
    "    - pandas\n",
    "    - cloudpickle\n",
    "    - pip\n",
    "    - pip:\n",
    "        - git+https://github.com/mlflow/mlflow\n",
    "        - sklearn\n",
    "        - pandas_datareader\n",
    "        - great-expectations\n",
    "        - pandas-profiling\n",
    "        - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main.py file\n",
    "_main = \"main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_main}\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "\n",
    "def _run(entrypoint, \n",
    "         parameters={}, \n",
    "         source_version=None, \n",
    "         use_cache=True):\n",
    "    print(\"---------------------\")\n",
    "    print(f\"Launching new run for entrypoint={entrypoint} and parameters={parameters}\")\n",
    "    submitted_run = mlflow.run(\".\", entrypoint, parameters=parameters)\n",
    "    return mlflow.tracking.MlflowClient().get_run(submitted_run.run_id)\n",
    "\n",
    "\n",
    "def workflow():\n",
    "    with mlflow.start_run(run_name =\"model-training\") as active_run:\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"model-training\")\n",
    "        \n",
    "        # ------\n",
    "        # Train model\n",
    "        train_run = _run(\"train_model\")\n",
    "        model_uri = os.path.join(train_run.info.artifact_uri, \"model\")\n",
    "        \n",
    "        print(f\"SUCCESS: Model {model_uri} trained.\")\n",
    "        \n",
    "        # ------\n",
    "        # Evaluate model\n",
    "        evaluate_run = _run(\"evaluate_model\")\n",
    "        print(f\"SUCCESS: Model {evaluate_run} validated.\")\n",
    "\n",
    "        # ------\n",
    "        # Register model   \n",
    "        #mlflow.register_model(model_uri, \"model-trained-evaluated\")\n",
    "        register_run = _run(\"register_model\", parameters={'model_uri': model_uri})\n",
    "        print(f\"SUCCESS: Model {register_run} registered.\")\n",
    "        \n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    workflow()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "Load the training data to fit and produce a model. Test predictions will be produced and persisted in the environment so that other steps of the workflow can use the data to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train_model.py file\n",
    "_train_model = \"train_model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_train_model}\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PATH_TO_CONFIG = \"/home/ksatola/work/src\"\n",
    "sys.path.insert(1, PATH_TO_CONFIG)\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import *\n",
    "\n",
    "\n",
    "def train_test_split_df(\n",
    "    pandas_df,\n",
    "    t_size=SPLIT_RATIO,\n",
    "    r_tate=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Docstring here...\n",
    "    \"\"\"\n",
    "    \n",
    "    X = pandas_df.iloc[:, :-1]\n",
    "    Y = pandas_df.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                        test_size=t_size, \n",
    "                                                        random_state=r_tate)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"train_model\") as run:\n",
    "        \n",
    "        mlflow.set_tag(\"mlflow.runName\", \"train_model\")\n",
    "\n",
    "        #df = pd.read_csv(\"/home/ksatola/work/data/training/data.csv\", header=None)\n",
    "        df = pd.read_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"training\", \"data.csv\"))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split_df(df)\n",
    "\n",
    "        train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "        test_data =  xgb.DMatrix(X_test)\n",
    "\n",
    "        model = xgb.train(dtrain=train_data, params={})\n",
    "        \n",
    "        y_hats = model.predict(test_data) \n",
    "        y_hats = [1 if y_hat > CLASS_THRESHOLD else 0. for y_hat in y_hats]\n",
    "\n",
    "        test_prediction_results = pd.DataFrame(data={\n",
    "            'y_pred':y_hats,\n",
    "            'y_test':y_test\n",
    "        })\n",
    "        \n",
    "        print(test_prediction_results)\n",
    "        \n",
    "        # Create the folder if does not exist\n",
    "        Path(PATH_TO_TRAINING, \"predictions\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #test_prediction_result.to_csv(\"/home/ksatola/work/data/predictions/test_predictions.csv\")\n",
    "        test_prediction_results.to_csv(os.path.join(PATH_TO_TRAINING, \"predictions\", \"test_predictions.csv\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce model performance metrics\n",
    "\n",
    "At this stage, we have our model saved and persisted on the artifacts of our MLflow installation. We will now move on to collect evaluation metrics for our model, to add to the metadata of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluate_model.py file\n",
    "_evaluate_model = \"evaluate_model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_evaluate_model}\n",
    "\n",
    "import sys\n",
    "PATH_TO_CONFIG = \"/home/ksatola/work/src\"\n",
    "sys.path.insert(1, PATH_TO_CONFIG)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    log_loss,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    zero_one_loss,\n",
    ")\n",
    "from config import *\n",
    "\n",
    "\n",
    "def classification_metrics(df:None):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy_score\"] = accuracy_score(df[\"y_pred\"], df[\"y_test\"])\n",
    "    metrics[\"average_precision_score\"] = average_precision_score(df[\"y_pred\"], df[\"y_test\"], average='micro')\n",
    "    metrics[\"f1_score\"] = f1_score(df[\"y_pred\"], df[\"y_test\"], average='micro')\n",
    "    metrics[\"jaccard_score\"] = jaccard_score( df[\"y_pred\"], df[\"y_test\"], average='micro')\n",
    "    metrics[\"log_loss\"] = log_loss(df[\"y_pred\"], df[\"y_test\"])\n",
    "    metrics[\"matthews_corrcoef\"] = matthews_corrcoef(df[\"y_pred\"], df[\"y_test\"])\n",
    "    metrics[\"precision_score\"] = precision_score(df[\"y_pred\"], df[\"y_test\"], average='micro')\n",
    "    metrics[\"recall_score\"] = recall_score(df[\"y_pred\"], df[\"y_test\"], average='micro')\n",
    "    metrics[\"zero_one_loss\"] = zero_one_loss(df[\"y_pred\"], df[\"y_test\"])\n",
    "    return metrics\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with mlflow.start_run(run_name=\"evaluate_model\") as run:\n",
    "        \n",
    "        mlflow.set_tag(\"mlflow.runName\", \"evaluate_model\")\n",
    "        \n",
    "        #df = pd.read_csv(\"/home/ksatola/work/data/predictions/test_predictions.csv\")\n",
    "        df = pd.read_csv(os.path.join(PATH_TO_TRAINING, \"predictions\", \"test_predictions.csv\"))\n",
    "        \n",
    "        metrics = classification_metrics(df)\n",
    "        \n",
    "        mlflow.log_metrics(metrics)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model\n",
    "\n",
    "Register the model in the Model Registry. A model will be created if it doesn't already exist. If it's already in the registry, a new version will be added, allowing the deployment tools to look at the models and trace the training jobs and metrics. Having this step separated, allows a decision to be made as to whether to promote the model to production or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the register_model.py file\n",
    "_register_model = \"register_model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_register_model}\n",
    "\n",
    "import mlflow\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model_uri = str(sys.argv[1])\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"register_model\") as run:\n",
    "\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"register_model\")\n",
    "\n",
    "        #result = mlflow.register_model(model_uri, \"model-trained-evaluated\")\n",
    "        mlflow.register_model(model_uri, \"SP_XGBoost\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Jupyter terminal\n",
    "cd /home/ksatola/work/data/training_pipeline\n",
    "\n",
    "mlflow run . --experiment-name=\"SP_Model_Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
