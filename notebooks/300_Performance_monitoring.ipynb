{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cf553d",
   "metadata": {},
   "source": [
    "# Performance Monitoring\n",
    "\n",
    "<img src=\"img/mlops_prediction_serving_and_monitoring.png\" width=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6a82e",
   "metadata": {},
   "source": [
    "## Stakeholders\n",
    "On the monitoring side of ML models, there are multiple interested parties, and we should take the requirements for monitoring from the different stakeholders involved. One example of a typical set of stakeholders is the following:\n",
    "- **Data scientists:** evaluating model performance and data drift that might negatively affect that performance.\n",
    "- **Software engineers:** metrics that assess whether their products have reliable and correct access to the APIs that are serving models.\n",
    "- **Data engineers:** ensure that the data pipelines are reliable and pushing data reliably, at the right velocity, and in line with the correct schemas.\n",
    "- **Business/product stakeholders:** interested in the core impact of the overall solution on their customer base.\n",
    "\n",
    "## Monitoring dimensions\n",
    "The most widely used dimensions of monitoring in the ML industry are the following:\n",
    "- **Data drift:** This corresponds to significant changes in the input data used either for training or inference in a model. It might indicate a change of the modeled premise in the real world, which will require the model to be retrained, redeveloped, or even archived if it's no longer suitable. This can be easily detected by monitoring the distributions of data used for training the model versus the data used for scoring or inference over time.\n",
    "- **Target drift:** In line with the change of regimens in input data, we often see the same change in the distribution of outcomes of the model over a period of time. The common periods are months, weeks, or days, and might indicate a significant change in the environment that would require model redevelopment and tweaking.\n",
    "- **Model (performance) drift:** This involves looking at whether the performance metrics such as accuracy for classification problems, or root mean square error, start suffering a gradually worsening over time. This is an indication of an issue with the model requiring investigation and action from the model developer or maintainer.\n",
    "- **Platform and infrastructure metrics:** This type of metrics is not directly related to modeling, but with the systems infrastructure that encloses the model. It implies abnormal CPU, memory, network, or disk usage that will certainly affect the ability of the model to deliver value to the business.\n",
    "- **Business metrics:** Very critical business metrics, such as the profitability of the models, in some circumstances should be added to the model operations in order to ensure that the team responsible for the model can monitor the ability of the model to deliver on its business premise.\n",
    "\n",
    "An emergent open source tool in the space of monitoring model performance is called [Evidently AI](https://evidentlyai.com/).\n",
    "\n",
    "Reference: https://github.com/evidentlyai/evidently/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03479616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in DS-Workbench Jupyter terminal\n",
    "# https://evidentlyai.com\n",
    "# pip install evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748be7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in DS-Workbench Jupyter terminal\n",
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from evidently.dashboard import Dashboard\n",
    "from evidently.dashboard.tabs import DataDriftTab, NumTargetDriftTab, CatTargetDriftTab, ClassificationPerformanceTab\n",
    "\n",
    "from data_utils import get_train_test_split_for_stock\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cc39a",
   "metadata": {},
   "source": [
    "Get a reference dataset, basically a training dataset. We will add a set of features to the pandas DataFrame so evidently will be able to use the feature names in the drift reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883ccff",
   "metadata": {},
   "source": [
    "## Reference dataset (used for model training, golden dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699aaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = pd.read_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"training\", \"data.csv\"))\n",
    "df_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c784bff",
   "metadata": {},
   "source": [
    "## Input data for evaluation\n",
    "These are input values for which we want to make predictions. We load input data for scoring with the intention to calculate the `distribution difference` between the data in the reference training set and the data to be scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd01a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(os.path.join(PATH_TO_DATA, \"performance_monitoring/input/input_data_for_evaluation.csv\"))\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no TARGET in this dataset\n",
    "df_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef9264",
   "metadata": {},
   "source": [
    "## Scored input dataset for evaluation\n",
    "This dataset is the input data for evaluation with predicted target values done with the model (input data for evaluation + predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d04f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_scored = pd.read_csv(os.path.join(PATH_TO_DATA, \"performance_monitoring/input/scored_input_data_for_evaluation.csv\"))\n",
    "df_input_scored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594b41d",
   "metadata": {},
   "source": [
    "## Set Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc20273",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('SP_Model_Monitoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d663c1c",
   "metadata": {},
   "source": [
    "## Data (input features) drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeadebb",
   "metadata": {},
   "source": [
    "Compare recent data with the past. Learn which features changed and if key models drivers shifted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Data drift\") as run:\n",
    "    \n",
    "    drift_dashboard = Dashboard(tabs=[DataDriftTab()])\n",
    "    drift_dashboard.calculate(df_ref, df_input_scored, column_mapping = None)\n",
    "    \n",
    "    drift_dashboard.save(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"input_data_drift.html\"))\n",
    "    drift_dashboard._save_to_json(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"input_data_drift.json\"))\n",
    "    \n",
    "    mlflow.log_artifact(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"input_data_drift.html\"))\n",
    "    mlflow.log_artifact(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"input_data_drift.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834afedf",
   "metadata": {},
   "source": [
    "## Target and prediction drift\n",
    "Understand how model predictions and target change over time. If the ground truth is delayed, catch the model decay in advance.\n",
    "\n",
    "Reference: https://evidentlyai.com/blog/evidently-014-target-and-prediction-drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Target drift\") as run:\n",
    "    \n",
    "    model_target_drift = Dashboard(tabs=[CatTargetDriftTab()])\n",
    "    model_target_drift.calculate(df_ref, df_input_scored)\n",
    "    \n",
    "    model_target_drift.save(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"target_drift.html\"))\n",
    "    model_target_drift._save_to_json(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"target_drift.json\"))\n",
    "    \n",
    "    mlflow.log_artifact(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"target_drift.html\"))\n",
    "    mlflow.log_artifact(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"target_drift.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db9e8a",
   "metadata": {},
   "source": [
    "## Model drift\n",
    "Monitoring model drift is extremely important to ensure that your model is still delivering at its optimal performance level. From this analysis, you can make a decision on whether to retrain your model or even develop a new one from scratch.\n",
    "\n",
    "Reference: https://evidentlyai.com/blog/evidently-018-classification-model-performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b72ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again, as something overrides original df (a bug?)\n",
    "df_ref = pd.read_csv(os.path.join(PATH_TO_DATA_PIPELINE, \"training\", \"data.csv\"))\n",
    "df_input_scored = pd.read_csv(os.path.join(PATH_TO_DATA, \"performance_monitoring/input/scored_input_data_for_evaluation.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference data\n",
    "X_train = df_ref.loc[:, df_ref.columns != 'target']\n",
    "y_train = df_ref.loc[:, df_ref.columns == 'target']\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197258b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from production (recent data)\n",
    "X_test = df_input_scored.iloc[:, :-1]\n",
    "y_test = df_input_scored.iloc[:, -1]\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56034386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logistig regression model\n",
    "logged_model = '/data/artifacts/3/135560bf2a324a609db4b0950d48fd5b/artifacts/model' # logistic reg\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model drift\") as run:\n",
    "    \n",
    "    train_proba_predict = model.predict(X_train) # reference\n",
    "    test_proba_predict = model.predict(X_test) # production\n",
    "    \n",
    "    train_predictions = [1. if y_cont > CLASS_THRESHOLD else 0. for y_cont in train_proba_predict]\n",
    "    test_predictions = [1. if y_cont > CLASS_THRESHOLD else 0. for y_cont in test_proba_predict]\n",
    "       \n",
    "    # Add target and prediction columns\n",
    "    ref_model_results = X_train.copy()\n",
    "    ref_model_results['target'] = y_train\n",
    "    ref_model_results['prediction'] = train_predictions\n",
    "    latest_model_results = X_test.copy()\n",
    "    latest_model_results['target'] = y_test\n",
    "    latest_model_results['prediction'] = test_predictions\n",
    "    \n",
    "    model_performance = Dashboard(tabs=[ClassificationPerformanceTab()])\n",
    "    model_performance.calculate(ref_model_results, latest_model_results)#, column_mapping=column_mapping)\n",
    "    \n",
    "    model_performance.save(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"model_drift.html\"))\n",
    "    model_performance._save_to_json(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"model_drift.json\"))\n",
    "    \n",
    "    mlflow.log_artifact(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"model_drift.html\"))\n",
    "    mlflow.log_artifact(os.path.join(PATH_TO_PERFORMANCE_REPORTS, \"model_drift.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85290f54",
   "metadata": {},
   "source": [
    "## Show \n",
    "- MLflow HTML report\n",
    "- /data/reports_data_drift/input_data_drift.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d5c67",
   "metadata": {},
   "source": [
    "## Infrastructure monitoring and alerting\n",
    "We should use `infrastructure monitoring tools`, like AWS CloudWatch, and then report to MLFlow.\n",
    "\n",
    "At a higher level, we can split the infrastructure monitoring and alerting components into the following three items:\n",
    "- **Resource metrics:** refers to metrics regarding the hardware infrastructure where the system is deployed (CPU utilization, memory utilization, network data transfer, disk I/O)\n",
    "- **System metrics:** refers to metrics regarding the system infrastructure where the system is deployed (request throughput, request latencies, validation metrics)\n",
    "- **Alerting:** For alerting, we use any of the metrics and set up a threshold that we consider acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a601ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
